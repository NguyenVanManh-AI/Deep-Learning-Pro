{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape       =  (49000, 3073)\n",
      "X_val shape         =  (1000, 3073)\n",
      "X_test shape        =  (10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "# Load CIFAR 10 dataset\n",
    "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# Tạo một tập validation từ X_train \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= 1000)\n",
    "# print('(New) X_train shape = ', X_train.shape)\n",
    "# print('X_val shape         = ', X_val.shape)\n",
    "\n",
    "# Feature engineering \n",
    "# mean image of all training images\n",
    "img_mean = np.mean(X_train, axis = 0)\n",
    "\n",
    "def feature_engineering(X):\n",
    "    X -= img_mean # zero-centered\n",
    "    N = X.shape[0] # number of data point \n",
    "    X = X.reshape(N, -1) # vectorizetion \n",
    "    return np.concatenate((X, np.ones((N, 1))), axis = 1) # bias trick \n",
    "\n",
    "X_train = feature_engineering(X_train)\n",
    "X_val = feature_engineering(X_val)\n",
    "X_test = feature_engineering(X_test)\n",
    "\n",
    "print('X_train shape       = ', X_train.shape)\n",
    "print('X_val shape         = ', X_val.shape)\n",
    "print('X_test shape        = ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idx = np.random.randint(0, X_train.shape[0], replace = False)\n",
    "n_dev = 10\n",
    "X_dev = X_train[:n_dev]\n",
    "y_dev = y_train[:n_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with reg = 0  : 34.1559661597\n",
      "Loss with reg = 0.1: 486.960068418\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# naive way to calculate loss and grad\n",
    "def svm_loss_naive(W, X, y, reg):\n",
    "    ''' calculate loss and gradient of the loss function at W\n",
    "    W: 2d numpy array of shape (d, C). The weight matrix.\n",
    "    X: 2d numpy array of shape (N, d). The training data \n",
    "    y: 1d numpy array of shape (N,). The training label\n",
    "    reg: a positive number. The regularization parameter\n",
    "    '''\n",
    "    d, C = W.shape \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    ## naive loss and grad\n",
    "    loss = 0 \n",
    "    dW = np.zeros_like(W)\n",
    "    for n in xrange(N):\n",
    "        xn = X[n]\n",
    "        score = xn.dot(W)\n",
    "        for j in xrange(C):\n",
    "            if j == y[n]:\n",
    "                continue \n",
    "            margin = 1 - score[y[n]] + score[j]\n",
    "            if margin > 0:\n",
    "                loss += margin \n",
    "                dW[:, j] += xn \n",
    "                dW[:, y[n]] -= xn\n",
    "    \n",
    "    loss /= N \n",
    "    loss += 0.5*reg*np.sum(W * W) \n",
    "    dW /= N \n",
    "    dW += reg*W\n",
    "    return loss, dW\n",
    "    \n",
    "# random, small data\n",
    "d, C, N = 3073, 3, 10\n",
    "reg = .1 \n",
    "W_rand = np.random.randn(d, C)\n",
    "X_rand = np.random.randn(N, d)\n",
    "y_rand = np.random.randint(0, C, N)\n",
    "\n",
    "# sanity check\n",
    "print('Loss with reg = 0  :', svm_loss_naive(W_rand, X_rand, y_rand, 0)[0])\n",
    "print('Loss with reg = 0.1:',svm_loss_naive(W_rand, X_rand, y_rand, .1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient difference: 0.000003\n"
     ]
    }
   ],
   "source": [
    "f = lambda W: svm_loss_naive(W, X_rand, y_rand, .1)[0]\n",
    "\n",
    "# for checking if calculated grad is correct\n",
    "def numerical_grad_general(W, f):\n",
    "    eps = 1e-6\n",
    "    g = np.zeros_like(W)\n",
    "    # flatening variable -> 1d. Then we need \n",
    "    # only one for loop\n",
    "    W_flattened = W.flatten()\n",
    "    g_flattened = np.zeros_like(W_flattened)\n",
    "    \n",
    "    for i in xrange(W.size):\n",
    "        W_p = W_flattened.copy()\n",
    "        W_n = W_flattened.copy()\n",
    "        W_p[i] += eps \n",
    "        W_n[i] -= eps \n",
    "        \n",
    "        # back to shape of W \n",
    "        W_p = W_p.reshape(W.shape)\n",
    "        W_n = W_n.reshape(W.shape)\n",
    "        g_flattened[i] = (f(W_p) - f(W_n))/(2*eps)\n",
    "        \n",
    "    # convert back to original shape\n",
    "    return g_flattened.reshape(W.shape) \n",
    "\n",
    "# compare two ways of computing gradient\n",
    "g1 = svm_loss_naive(W_rand, X_rand, y_rand, .1)[1]\n",
    "g2 = numerical_grad_general(W_rand, f)\n",
    "print('gradient difference: %f' %np.linalg.norm(g1 - g2)) # this should be very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more efficient way to compute loss and grad\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    d, C = W.shape \n",
    "    N = X.shape[0]\n",
    "    loss = 0 \n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    Z = X.dot(W) # shape of (N, C)\n",
    "    id0 = np.arange(Z.shape[0])\n",
    "    correct_class_score = Z[id0, y].reshape(N, 1) # shape of (N, 1)\n",
    "    margins = np.maximum(0, Z - correct_class_score + 1) # shape of (N, C)\n",
    "    margins[id0, y] = 0\n",
    "    loss = np.sum(margins)\n",
    "    loss /= N \n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    \n",
    "    F = (margins > 0).astype(int)# shape of (N, C)\n",
    "    F[np.arange(F.shape[0]), y] = np.sum(-F, axis = 1)\n",
    "    dW = X.T.dot(F)/N + reg*W\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive      -- run time: 7.34640693665 (s)\n",
      "Vectorized -- run time: 0.365024089813 (s)\n",
      "loss difference: 8.73114913702e-11\n",
      "gradient difference: 1.87942037251e-10\n"
     ]
    }
   ],
   "source": [
    "d, C = 3073, 10\n",
    "reg = .1 \n",
    "W_rand = np.random.randn(d, C)\n",
    "import time \n",
    "t1 = time.time()\n",
    "l1, dW1 = svm_loss_naive(W_rand, X_train, y_train, reg)\n",
    "t2 = time.time()\n",
    "print('Naive      -- run time:', t2 - t1, '(s)')\n",
    "\n",
    "t1 = time.time()\n",
    "l2, dW2 = svm_loss_vectorized(W_rand, X_train, y_train, reg)\n",
    "t2 = time.time()\n",
    "print('Vectorized -- run time:', t2 - t1, '(s)')\n",
    "print('loss difference:', np.linalg.norm(l1 - l2))\n",
    "print('gradient difference:', np.linalg.norm(dW1 - dW2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 3073)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5/50, loss = 5.482782\n",
      "epoch 10/50, loss = 5.204365\n",
      "epoch 15/50, loss = 4.885159\n",
      "epoch 20/50, loss = 5.051539\n",
      "epoch 25/50, loss = 5.060423\n",
      "epoch 30/50, loss = 4.691241\n",
      "epoch 35/50, loss = 4.841132\n",
      "epoch 40/50, loss = 4.643097\n",
      "epoch 45/50, loss = 4.691177\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch gradient descent\n",
    "def multiclass_svm_GD(X, y, Winit, reg, lr=.1, \\\n",
    "        batch_size = 1000, num_iters = 50, print_every = 10):\n",
    "    W = Winit \n",
    "    loss_history = []\n",
    "    for it in xrange(num_iters):\n",
    "        mix_ids = np.random.permutation(X.shape[0])\n",
    "        n_batches = int(np.ceil(X.shape[0]/float(batch_size)))\n",
    "        for ib in range(n_batches):\n",
    "            ids = mix_ids[batch_size*ib: min(batch_size*(ib+1), X.shape[0])]\n",
    "            X_batch = X[ids]\n",
    "            y_batch = y[ids]\n",
    "            lossib, dW = svm_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "            loss_history.append(lossib)\n",
    "            W -= lr*dW \n",
    "        if it % print_every == 0 and it > 0:\n",
    "           print('epoch %d/%d, loss = %f' %(it, num_iters, loss_history[-1]))\n",
    "    return W, loss_history \n",
    "\n",
    "d, C = X_train.shape[1], 10\n",
    "reg = .1 \n",
    "W = 0.00001*np.random.randn(d, C)\n",
    "\n",
    "W, loss_history = multiclass_svm_GD(X_train, y_train, W, reg, lr = 1e-8, num_iters = 50, print_every = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAENCAYAAADnrmWtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDFZyIK/McZkIQv+xhiThSz4G2NMFvr/BeVK+EWgFaMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10af74890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# plot loss as a function of iteration\n",
    "with PdfPages('loss_history.pdf') as pdf:\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('number of iterations', fontsize = 13)\n",
    "    plt.ylabel('loss function', fontsize = 13)\n",
    "    pdf.savefig()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multisvm_predict(W, X):\n",
    "    Z = X.dot(W)\n",
    "    return np.argmax(Z, axis=1)\n",
    "\n",
    "def evaluate(W, X, y):\n",
    "    y_pred = multisvm_predict(W, X)\n",
    "    acc = 100*np.mean(y_pred == y)\n",
    "    return acc \n",
    "# y_pred = multisvm_predict(W, X_test)\n",
    "# acc = 100*np.mean(y_pred == y_test)\n",
    "# print('training accuracy: %.2f %%' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1.000000e-09, reg = 1.000000e-01, loss = 4.422479, validation acc = 40.30\n",
      "lr = 1.000000e-09, reg = 1.000000e-02, loss = 4.474095, validation acc = 40.70\n",
      "lr = 1.000000e-09, reg = 1.000000e-03, loss = 4.240144, validation acc = 40.90\n",
      "lr = 1.000000e-09, reg = 1.000000e-04, loss = 4.257436, validation acc = 41.40\n",
      "lr = 1.000000e-08, reg = 1.000000e-01, loss = 4.482856, validation acc = 41.50\n",
      "lr = 1.000000e-08, reg = 1.000000e-02, loss = 4.036566, validation acc = 41.40\n",
      "lr = 1.000000e-08, reg = 1.000000e-03, loss = 4.085053, validation acc = 41.00\n",
      "lr = 1.000000e-08, reg = 1.000000e-04, loss = 3.891934, validation acc = 41.40\n",
      "lr = 1.000000e-07, reg = 1.000000e-01, loss = 3.947408, validation acc = 41.50\n",
      "lr = 1.000000e-07, reg = 1.000000e-02, loss = 4.088984, validation acc = 41.90\n",
      "lr = 1.000000e-07, reg = 1.000000e-03, loss = 4.073365, validation acc = 41.70\n",
      "lr = 1.000000e-07, reg = 1.000000e-04, loss = 4.006863, validation acc = 41.80\n",
      "lr = 1.000000e-06, reg = 1.000000e-01, loss = 3.851727, validation acc = 41.90\n",
      "lr = 1.000000e-06, reg = 1.000000e-02, loss = 3.941015, validation acc = 41.80\n",
      "lr = 1.000000e-06, reg = 1.000000e-03, loss = 3.995598, validation acc = 41.60\n",
      "lr = 1.000000e-06, reg = 1.000000e-04, loss = 3.857822, validation acc = 41.80\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "regs = [0.1, 0.01, 0.001, 0.0001]\n",
    "best_W = 0\n",
    "best_acc = 0\n",
    "for lr in lrs:\n",
    "    for reg in regs: \n",
    "        W, loss_history = multiclass_svm_GD(X_train, y_train, W, reg, \\\n",
    "                                            lr = 1e-8, num_iters = 100, print_every = 1e20)\n",
    "        acc = evaluate(W, X_val, y_val)\n",
    "        print('lr = %e, reg = %e, loss = %f, validation acc = %.2f' %(lr, reg, loss_history[-1], acc))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc \n",
    "            best_W = W \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3073, 10)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data =  39.88\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(best_W, X_test, y_test)\n",
    "print('Accuracy on test data = %2f %%'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1.000000e-07, reg = 1.000000e-04, loss = 4.432414, validation acc = 39.90\n",
      "lr = 1.000000e-07, reg = 1.000000e-05, loss = 4.110013, validation acc = 40.10\n",
      "lr = 1.000000e-07, reg = 1.000000e-06, loss = 4.212654, validation acc = 40.80\n",
      "lr = 1.000000e-06, reg = 1.000000e-04, loss = 4.240339, validation acc = 41.30\n",
      "lr = 1.000000e-06, reg = 1.000000e-05, loss = 4.295307, validation acc = 41.00\n",
      "lr = 1.000000e-06, reg = 1.000000e-06, loss = 4.209549, validation acc = 41.20\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-7, 1e-6]\n",
    "regs = [0.0001, 1e-5, 1e-6]\n",
    "best_W = 0\n",
    "best_acc = 0\n",
    "for lr in lrs:\n",
    "    for reg in regs: \n",
    "        W, loss_history = multiclass_svm_GD(X_train, y_train, W, reg, \\\n",
    "                                            lr = 1e-8, num_iters = 100, print_every = 1e20)\n",
    "        acc = evaluate(W, X_val, y_val)\n",
    "        print('lr = %e, reg = %e, loss = %f, validation acc = %.2f' %(lr, reg, loss_history[-1], acc))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc \n",
    "            best_W = W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 %\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "print('%d %%' %a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
