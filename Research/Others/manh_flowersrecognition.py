# -*- coding: utf-8 -*-
"""Manh_FlowersRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16MbzrynKLE48PXnhTt78TlIfxKm6vUOQ

## Init
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -q efficientnet

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# import seaborn as sns
# import pickle
import scipy.io
import tarfile
# import csv
# # import sys
import os
import tensorflow as tf
# import tensorflow.keras as keras
import tensorflow.keras.models as M
import tensorflow.keras.layers as L
import tensorflow.keras.backend as K
import tensorflow.keras.callbacks as C
# from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import (
    LearningRateScheduler,
    ModelCheckpoint,
    EarlyStopping
)
# from tensorflow.keras.callbacks import Callback
from tensorflow.keras import optimizers
import efficientnet.tfkeras as efn
from sklearn.model_selection import train_test_split
import PIL
# from PIL import ImageOps, ImageFilter

os.chdir("/content/drive/MyDrive/3_XuLyAnh_Flowers")
os.getcwd()

EPOCHS        = 5
BATCH_SIZE    = 8
LR_step1      = 1e-3
LR_step2      = 1e-4
LR_step3      = 1e-5
VAL_SPLIT     = 0.2
CLASS_NUM     = 15       #102
IMG_SIZE      = 250    # (725*500)
IMG_CHANNELS  = 3
input_shape   = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)
RANDOM_SEED   = 42

"""## Data

### Load Data
"""

# # Đường dẫn đến tệp .tgz cần giải nén
# file_path = os.path.join(os.getcwd(), '102flowers.tgz')

# # Mở tệp .tgz
# with tarfile.open(file_path, 'r:gz') as tar:
#     # Giải nén toàn bộ nội dung của tệp .tgz vào thư mục hiện tại
#     tar.extractall()

DATA_PATH = os.getcwd()
PATH = os.getcwd() + "/jpg/"

def get_all_filenames(tar_fn):
    with tarfile.open(tar_fn) as f:
        return [m.name for m in f.getmembers() if m.isfile()]


df = pd.DataFrame()
df['Id'] = sorted(get_all_filenames(DATA_PATH + '/102flowers.tgz'))

# load file label:  label (1 -> 102) == -1 ==>  (0 -> 101)
df['Category'] = scipy.io.loadmat(DATA_PATH + '/imagelabels.mat')['labels'][0] - 1
df['Category'] = df['Category'].astype(str)

top_15_categories = ['48', '76', '42', '72', '89', '73', '81', '95', '88', '77', '37', '39', '83', '96', '74']

# Lọc DataFrame ban đầu
df_15 = df[df['Category'].isin(top_15_categories)]

# df.head(5)

"""### Visualization Data"""

# Đoạn mã để đếm giá trị của cột "Category" và vẽ biểu đồ cột
df_15['Category'].value_counts().plot(kind='bar')
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Count of Each Category')
plt.show()

# # Đoạn mã để đếm giá trị của cột "Category" và vẽ biểu đồ cột
# df['Category'].value_counts().plot(kind='bar')
# plt.xlabel('Category')
# plt.ylabel('Count')
# plt.title('Count of Each Category')
# plt.show()

print(f"Có {df_15['Category'].nunique()} loài hoa")
print("Giá trị lớn nhất của cột 'Category':", df['Category'].value_counts().max())
print("Giá trị nhỏ nhất của cột 'Category':", df['Category'].value_counts().min())

# print(f"Có {df['Category'].nunique()} loài hoa")
# print("Giá trị lớn nhất của cột 'Category':", df['Category'].value_counts().max())
# print("Giá trị nhỏ nhất của cột 'Category':", df['Category'].value_counts().min())

df_15['Category'].unique()   # giá trị từ 1 đến 101

# df['Category'].unique()   # giá trị từ 1 đến 101

PATH = os.getcwd() + "/"

print('Random Sample')
plt.figure(figsize = (12, 8))
random_image = df_15.sample(n = 9)
random_image_paths = random_image['Id'].values
random_image_cat = random_image['Category'].values

for index, path in enumerate(random_image_paths):
    im = PIL.Image.open(PATH + path)
    plt.subplot(3, 3, index + 1)
    plt.imshow(im)
    plt.title('Class: ' + str(random_image_cat[index]))
    plt.axis('off')

plt.show()

# print('Random Sample')
# plt.figure(figsize = (12, 8))
# random_image = df.sample(n = 9)
# random_image_paths = random_image['Id'].values
# random_image_cat = random_image['Category'].values

# for index, path in enumerate(random_image_paths):
#     im = PIL.Image.open(PATH + path)
#     plt.subplot(3, 3, index + 1)
#     plt.imshow(im)
#     plt.title('Class: ' + str(random_image_cat[index]))
#     plt.axis('off')

# plt.show()

image = PIL.Image.open(PATH + path)
imgplot = plt.imshow(image)
plt.show()
image.size

"""### Split Train Test"""

train_files, test_files, train_labels, test_labels = \
    train_test_split(
        df_15['Id'],
        df_15['Category'],
        test_size = 0.2,
        random_state = 42,
        stratify = df_15['Category']
)
train_files = pd.DataFrame(train_files)
test_files = pd.DataFrame(test_files)
train_files['Category'] = train_labels
test_files['Category'] = test_labels
train_files.shape, test_files.shape

# train_files, test_files, train_labels, test_labels = \
#     train_test_split(
#         df['Id'],
#         df['Category'],
#         test_size = 0.2,
#         random_state = 42,
#         stratify = df['Category']
# )
# train_files = pd.DataFrame(train_files)
# test_files = pd.DataFrame(test_files)
# train_files['Category'] = train_labels
# test_files['Category'] = test_labels
# train_files.shape, test_files.shape

train_files.head(5)

# Đoạn mã để đếm giá trị của cột "Category" và vẽ biểu đồ cột
train_files['Category'].value_counts().plot(kind='bar')
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Count of Each Category on Train Data')
plt.show()

# Đoạn mã để đếm giá trị của cột "Category" và vẽ biểu đồ cột
test_files['Category'].value_counts().plot(kind='bar')
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Count of Each Category on Test Data')
plt.show()

"""### Data augmentation"""

train_datagen = ImageDataGenerator(
    rescale = 1. / 255,
    rotation_range = 50,
    shear_range = 0.2,
    zoom_range = [0.75, 1.25],
    brightness_range = [0.5, 1.5],
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    horizontal_flip = True
)
test_datagen = ImageDataGenerator(rescale = 1. / 255)

train_generator = train_datagen.flow_from_dataframe(
    dataframe = train_files,
    directory = DATA_PATH,
    x_col = 'Id',
    y_col = 'Category',
    target_size = (IMG_SIZE, IMG_SIZE),
    batch_size = BATCH_SIZE,
    class_mode = 'categorical',
    shuffle = True,
    seed = RANDOM_SEED
)
test_generator = test_datagen.flow_from_dataframe(
    dataframe = test_files,
    directory = DATA_PATH,
    x_col = 'Id',
    y_col = 'Category',
    target_size = (IMG_SIZE, IMG_SIZE),
    batch_size = BATCH_SIZE,
    class_mode = 'categorical',
    shuffle = False,
    seed = RANDOM_SEED
)

from skimage import io

def imshow(image_RGB):
    io.imshow(image_RGB)
    io.show()

x, y = train_generator.next()
print('Ví dụ về hình ảnh từ train_generator')
plt.figure(figsize = (12, 8))

for i in range(0, 6):
    image = x[i]
    plt.subplot(3, 3, i + 1)
    plt.imshow(image)
#     plt.title('Class: ' + str(y[i]))
#     plt.axis('off')

plt.show()

x, y = test_generator.next()
print('Ví dụ về hình ảnh từ test_generator')
plt.figure(figsize = (12, 8))

for i in range(0, 6):
    image = x[i]
    plt.subplot(3, 3, i + 1)
    plt.imshow(image)
#     plt.title('Class: ' + str(y[i]))
#     plt.axis('off')

plt.show()

lenet5_model = M.Sequential()
lenet5_model.add(L.Conv2D(6, 5, activation='tanh', input_shape=input_shape))
lenet5_model.add(L.AveragePooling2D(2))
lenet5_model.add(L.Activation('sigmoid'))
lenet5_model.add(L.Conv2D(16, 5, activation='tanh'))
lenet5_model.add(L.AveragePooling2D(2))
lenet5_model.add(L.Activation('sigmoid'))
lenet5_model.add(L.Conv2D(6, 5, activation='tanh'))
lenet5_model.add(L.Flatten())
lenet5_model.add(L.Dense(2048, activation='tanh'))
# lenet5_model.add(L.Dense(10, activation='softmax'))

# Print model summary
lenet5_model.summary()

model = M.Sequential()
model.add(lenet5_model)
# model.add(L.GlobalAveragePooling2D())
model.add(L.Dense(CLASS_NUM, activation = 'softmax'))

model.summary()

print(len(model.layers))
print(len(model.trainable_variables))

# check the trainable status of the individual layers
for layer in model.layers:
    print(layer, layer.trainable)

model.compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizers.Adam(learning_rate = LR_step1),
    metrics = ['accuracy']
)

"""earlystop:
- monitor - giám sát trên thang đo val_acc
- patience - số lượng epochs không cải thiện độ chính xác sẽ dừng
- restore_best_weights - quay lại trọng số cho độ chính xác tốt nhất
"""

checkpoint = ModelCheckpoint(
    'best_model_test.hdf5',
    monitor = ['val_accuracy'],
    verbose = 1,
    mode = 'max'
)
earlystop = EarlyStopping(
    monitor = 'val_accuracy',
    patience = 10,
    restore_best_weights = True
)
callbacks_list = [checkpoint, earlystop]

# scores = model.evaluate(test_generator, verbose=1)
# print('Accuracy: %.2f%%' % (scores[1] * 100))

"""### Huấn luyện

đã huấn luyện được 12 epoch
"""

# fit include generator
history = model.fit(
    train_generator,
    steps_per_epoch = train_generator.samples // train_generator.batch_size,
    validation_data = test_generator,
    validation_steps = test_generator.samples // test_generator.batch_size,
    epochs = 50,
    callbacks = callbacks_list
)

model.save('./SaveModel/model_lenet5_step1.hdf5')
# model.load_weights('best_model_test.hdf5')

"""## Cur"""

# scores = model.evaluate_generator(test_generator, verbose = 1)
scores = model.evaluate(test_generator, verbose=1)
print('Accuracy: %.2f%%' % (scores[1] * 100))

def plot_history(history):
    plt.figure(figsize = (10, 5))
#     plt.style.use('dark_background')
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(len(acc))
    plt.plot(epochs, acc, 'b', label = 'Training acc')
    plt.plot(epochs, val_acc, 'g', label = 'Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
#     plt.figure()
    plt.figure(figsize = (10, 5))
#     plt.style.use('dark_background')
    plt.plot(epochs, loss, 'b', label = 'Training loss')
    plt.plot(epochs, val_loss, 'g', label = 'Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()

plot_history(history)

### AlexNet

model_alexnet = M.Sequential()
model_alexnet.add(L.experimental.preprocessing.Resizing(224, 224, interpolation="bilinear", input_shape=input_shape))
model_alexnet.add(L.Conv2D(96, 11, strides=4, padding='same'))
model_alexnet.add(L.Lambda(tf.nn.local_response_normalization))
model_alexnet.add(L.Activation('relu'))
model_alexnet.add(L.MaxPooling2D(3, strides=2))
model_alexnet.add(L.Conv2D(256, 5, strides=4, padding='same'))
model_alexnet.add(L.Lambda(tf.nn.local_response_normalization))
model_alexnet.add(L.Activation('relu'))
model_alexnet.add(L.MaxPooling2D(3, strides=2))
model_alexnet.add(L.Conv2D(384, 3, strides=4, padding='same'))
model_alexnet.add(L.Activation('relu'))
model_alexnet.add(L.Conv2D(384, 3, strides=4, padding='same'))
model_alexnet.add(L.Activation('relu'))
model_alexnet.add(L.Conv2D(256, 3, strides=4, padding='same'))
model_alexnet.add(L.Activation('relu'))
model_alexnet.add(L.Flatten())
model_alexnet.add(L.Dense(4096, activation='relu'))
model_alexnet.add(L.Dropout(0.5))
model_alexnet.add(L.Dense(4096, activation='relu'))
model_alexnet.add(L.Dropout(0.5))
model_alexnet.add(L.Dense(10, activation='softmax'))
model_alexnet.summary()

model = M.Sequential()
model.add(model_alexnet)
# model.add(L.GlobalAveragePooling2D())
model.add(L.Dense(CLASS_NUM, activation = 'softmax'))

model.summary()

print(len(model.layers))
print(len(model.trainable_variables))

# check the trainable status of the individual layers
for layer in model.layers:
    print(layer, layer.trainable)

model.compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizers.Adam(learning_rate = LR_step1),
    metrics = ['accuracy']
)

"""### Huấn luyện"""

checkpoint = ModelCheckpoint(
    'best_model_test_2.hdf5',
    monitor = ['val_accuracy'],
    verbose = 1,
    mode = 'max'
)
earlystop = EarlyStopping(
    monitor = 'val_accuracy',
    patience = 10,
    restore_best_weights = True
)
callbacks_list = [checkpoint, earlystop]

# fit include generator
history = model.fit(
    train_generator,
    steps_per_epoch = train_generator.samples // train_generator.batch_size,
    validation_data = test_generator,
    validation_steps = test_generator.samples // test_generator.batch_size,
    epochs = 50,
    callbacks = callbacks_list
)

"""## this"""

model.save('./SaveModel/model_alexnet_step1.hdf5')
# model.load_weights('best_model_test_2.hdf5')

# scores = model.evaluate_generator(test_generator, verbose = 1)
scores = model.evaluate(test_generator, verbose=1)
print('Accuracy: %.2f%%' % (scores[1] * 100))

plot_history(history)

"""## Model Transfer Learning EfficientNetB6"""

input_shape

base_model = efn.EfficientNetB6(
    weights = 'imagenet',
    include_top = False,
    input_shape = input_shape
)

base_model.summary()

base_model.trainable = False

model = M.Sequential()
model.add(base_model)
model.add(L.GlobalAveragePooling2D())
model.add(L.Dense(CLASS_NUM, activation = 'softmax'))

model.summary()

print(len(model.layers))

len(model.trainable_variables)

# check the trainable status of the individual layers
for layer in model.layers:
    print(layer, layer.trainable)

"""### Step 1: Freeze Base Layer + Training new Head Classifier"""

model.compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizers.Adam(learning_rate = LR_step1),
    metrics = ['accuracy']
)

checkpoint = ModelCheckpoint(
    'best_model.hdf5',
    monitor = ['val_accuracy'],
    verbose = 1,
    mode = 'max'
)
earlystop = EarlyStopping(
    monitor = 'val_accuracy',
    patience = 5,
    restore_best_weights = True
)
callbacks_list = [checkpoint, earlystop]

"""Đánh giá trước khi huấn luyện"""

# scores = model.evaluate_generator(test_generator, verbose = 1)
scores = model.evaluate(test_generator, verbose=1)
print('Accuracy: %.2f%%' % (scores[1] * 100))

print(train_generator.samples)

# fit include generator
history = model.fit(
    train_generator,
    steps_per_epoch = train_generator.samples // train_generator.batch_size,
    validation_data = test_generator,
    validation_steps = test_generator.samples // test_generator.batch_size,
    epochs = 5,
    callbacks = callbacks_list
)

model.save('./SaveModel/model_step1.hdf5')
model.load_weights('best_model.hdf5')

"""Đánh giá sau khi huấn luyện"""

# scores = model.evaluate_generator(test_generator, verbose = 1)
scores = model.evaluate(test_generator, verbose=1)
print('Accuracy: %.2f%%' % (scores[1] * 100))

"""Visualizer training history"""

def plot_history(history):
    plt.figure(figsize = (10, 5))
#     plt.style.use('dark_background')
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(len(acc))
    plt.plot(epochs, acc, 'b', label = 'Training acc')
    plt.plot(epochs, val_acc, 'g', label = 'Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
#     plt.figure()
    plt.figure(figsize = (10, 5))
#     plt.style.use('dark_background')
    plt.plot(epochs, loss, 'b', label = 'Training loss')
    plt.plot(epochs, val_loss, 'g', label = 'Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()


plot_history(history)

"""### Step 2: Freeze a half layers of base_model and fine-turning"""

# let's take a look to see how many layers are in the base model
print('Number of layers in the base model:', len(base_model.layers))

base_model.trainable = True

# fine-tune from this layer onwards
fine_tune_at = len(base_model.layers) // 2

# freeze all the layers before the 'fine_tune_at' layer
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

len(base_model.trainable_variables)

# check the trainable status of the individual layers
for layer in model.layers:
    print(layer, layer.trainable)

model.compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizers.Adam(learning_rate = LR_step2),
    metrics = ['accuracy']
)

model.summary()

scores = model.evaluate(test_generator, verbose=1)
print('Accuracy: %.2f%%' % (scores[1] * 100))

history = model.fit(
    train_generator,
    steps_per_epoch = train_generator.samples // train_generator.batch_size,
    validation_data = test_generator,
    validation_steps = test_generator.samples // test_generator.batch_size,
    epochs = 10,
    callbacks = callbacks_list
)

model.save('./SaveModel/model_step2.hdf5')
model.load_weights('best_model.hdf5')

scores = model.evaluate(test_generator, verbose = 1)
print('Accuracy: %.2f%%' % (scores[1] * 100))

plot_history(history)

"""### Step 3: No Freeze Base model and full fine-turning"""

base_model.trainable = True

model.compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizers.Adam(lr = LR_step3),
    metrics = ['accuracy']
)

history = model.fit(
    train_generator,
    steps_per_epoch = train_generator.samples // train_generator.batch_size,
    validation_data = test_generator,
    validation_steps = test_generator.samples // test_generator.batch_size,
    epochs = 10,
    callbacks = callbacks_list
)

model.save('./SaveModel/model_step3.hdf5')
model.load_weights('best_model.hdf5')

scores = model.evaluate_generator(test_generator, verbose = 1)
print('Accuracy: %.2f%%' % (scores[1] * 100))

"""## Final Score"""

from sklearn.metrics import accuracy_score

model.load_weights('best_model.hdf5')

predictions = model.predict(test_generator, verbose = 1)
predictions = np.argmax(predictions, axis = -1)         # multiple categories
label_map = (train_generator.class_indices)
label_map = dict((v, k) for k, v in label_map.items())  # flip k, v
predictions = [label_map[k] for k in predictions]

filenames_with_dir = test_generator.filenames
submission = pd.DataFrame(
    {'Predict': predictions},
    columns = ['Predict'],
    index = filenames_with_dir
)
test_files.index = test_files['Id']
tmp_y = pd.concat(
    [submission['Predict'], test_files['Category']],
    axis = 1,
    sort = False
)
tmp_y.head(5)

print('Accuracy: %.2f%%' % (
    accuracy_score(tmp_y['Category'], tmp_y['Predict']) * 100
))

"""### confusion matrix"""

from sklearn.metrics import confusion_matrix

# tmp_y['Category'] là nhãn thực tế, tmp_y['Predict'] là dự đoán của mô hình
conf_matrix = confusion_matrix(tmp_y['Category'], tmp_y['Predict'])

print("Confusion Matrix:")
print(conf_matrix)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Vẽ heatmap cho confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# In ra các trường hợp nhầm lẫn
for i in range(len(conf_matrix)):
    for j in range(len(conf_matrix)):
        if i != j and conf_matrix[i][j] > 0:
            print("Thực tế:", i, "Dự đoán:", j, "Số lượng:", conf_matrix[i][j])

# Tính tổng số lượng nhầm lẫn cho từng loại
errors_per_category = conf_matrix.sum(axis=1)

# Xây dựng bảng thống kê
error_table = []
for i, error_count in enumerate(errors_per_category):
    error_table.append({'Loại': i, 'Số lượng nhầm lẫn': error_count})

# Sắp xếp bảng thống kê theo số lượng nhầm lẫn giảm dần
error_table.sort(key=lambda x: x['Số lượng nhầm lẫn'], reverse=True)

# In bảng thống kê
print("Bảng thống kê số loại bị nhầm lẫn nhiều nhất từ cao đến thấp:")
for item in error_table:
    print("Loại:", item['Loại'], "- Số lượng nhầm lẫn:", item['Số lượng nhầm lẫn'])

# Lấy thông tin để vẽ biểu đồ
categories = [str(item['Loại']) for item in error_table[0:15]]
errors = [item['Số lượng nhầm lẫn'] for item in error_table[0:15]]

# Vẽ biểu đồ
plt.figure(figsize=(10, 6))
plt.bar(categories, errors, color='skyblue')
plt.xlabel('Loại')
plt.ylabel('Số lượng nhầm lẫn')
plt.title('Số lượng loại bị nhầm lẫn nhiều nhất từ cao đến thấp cho từng loại')
plt.xticks(categories)
plt.show()

"""Category: ['48', '76', '42', '72', '89', '73', '81', '95', '88', '77', '37', '39', '83', '96', '74']"""

print(categories)

"""Đây là các 15 loại nhầm lẫn nhiều nhất.
Công việc tiếp theo:
- Tính ra % chính xác của model efficientnetB6 với 15 loại này (dự đoán sẽ giảm so với 84% trên toàn bộ tập dữ liệu)
- Cắt bộ dữ liệu ra 15 loại
- Huấn luyện trên model Lenet5 với 37tr tham số, Alexnet, GoogleNet và thống kê kết quả

tính độ chính xác của 15 loại trên best model efficientNetB6:
- phải huấn luyện lại mô hình trên 15 loại này
- rồi đánh giá lại, mà huấn luyện rất mất thời gian, chắc là dùng lại model hiện giờ luôn.
"""